---
layout: post
title:  "softmax for large language model"
categories: jekyll update
published: true

---
### Softmax: from number to distribution
Softmax is a differentiable version of max function, that normalize vector into a distribution by   
- preserve relative ordering of elements
- resulting non-negative values that sum to one

It's one of the most critical and ubiquitous function in deep learning, because it bridges the gap between linear transformation of features and posterior distribution for classification. Its presence in attention mechanisms and the great success of transformer further increase its occurrence to the same level as linear transformation or layer normalization. As a result, the marginal gain from accelerating softmax gets significant.

### Numerical stable softmax
Softmax is calculated as 

$$
\text{Softmax}(X)_i=\frac{\exp(x_i)}{\sum \exp(x_i)}
$$

Although softmax is enjoying positive range of exponential, numerical limitation of exponential follows. Exponential can easily become zero or go beyond range of any floating point number type supported natively by any machine, and the division bring calculation to `NaN`.

Luckily, the exponential function is a homomorphism between addition and multiplication, preserving structure under translation.
A numerical stable version of softmax is

$$
\text{Softmax}(X)_i=\frac{\exp(x_i)/\exp(\max(X))}{\sum \exp(x_i)/\exp(\max(X))}=\frac{\exp(x_i-\max(X))}{\sum \exp(x_i-\max(X))}
$$

Which can be computed by 
``` python
def softmax(x: torch.Tensor, dim):
    maxx = x.max(dim, keepdim=True).values
    ex = torch.exp(x - maxx)
    sumex = ex.sum(dim, keepdim=True)
    smx = ex / sumex
    return smx
```
### Cost of softmax
On modern computing hardware, especially GPUs, compute throughput (number of compute instructions executed per unit time) is way more than memory bandwidth. Nvidia H200 feature 989 TFLOPS of compute throughput on TP32 data at sparse mode. In scanning scenario, it takes roughly

$$
\textcolor{blue}{1000\text{ TFLOPS} } 
\times\textcolor{red}{4 \text{ Byte}}
=  4000 \text{TB/s}
$$

of memory bandwidth to keep tensor unit busy. But in reality, memory bandwidth of H200 is 4.8TB/s. The 1000x throughput gap highlight the importance of memory access optimization.

The softmax implementation above have `3MN + 2M` loads and `2MN+2M` stores. 
``` python
def softmax(x: torch.Tensor, dim):
    # read: M*N, write: M if dim=1
    maxx = x.max(dim, keepdim=True).values
    # read: M*N + M, write: M * N # read traffic can be hidden into sum
    ex = torch.exp(x - maxx)
    # read: M*N, write: M
    sumex = ex.sum(dim, keepdim=True)
    # read: M*N + M, write: M * N
    smx = ex / sumex
    return smx
```
In a real-world scenario like deepseekv3[^1], there are `61` transformer layers, `128` attention heads per layer. The batch size is `480` and of sequence length `128k`. Softmax is applied in multi-head attention, with input shape of `[batchsize, num_head, seq_len, seqlen]`. The input to softmax can be seen as 2d tensor of shape `128k, 128k` and type BF16[^2]. As a result, memory for input tensor per element per attention head is `16G * 2B = 32GB`. A single scanning pass takes `32GB`. Load traffic becomes `96GB` and store traffic becomes `64GB` if we ignore single order term `M`. 


### Online softmax

Currently, it takes 3 scanning passes: max, sum, and division. The calculation above shows `32GB` of traffic can be avoided if we somehow remove a pass. Online softmax[^3] have been proposed to achieve exactly this. It managed to compute both `max` and `sum(exp(x - max))` in single pass.

Online softmax approach the problem in dynamic programming manner. Given

$$
m_n=\max(X_{\leq n}) \text{ and }

s_{n}=\sum_{i=0}^n{\exp(x_i - m_n)}
$$

We can include element i+1 by

$$
\begin{aligned}
m_{i+1} &\leftarrow \max(m_i, x_{i+1})\\
s_{i+1} &\leftarrow s_{i} \times \exp(m_{i+1} - m_i) + \exp(x_{i+1} - m_{i+1})
\end{aligned}
$$

And the resulting pseudo-code is
``` python
def online_softmax(x: torch.Tensor, dim):
    # read: M*N, write: 2M if dim=1
    maxx, sumex = x.max_sumex(dim, keepdim=True)
    # read: M*N + 2M, write: M * N
    smx = torch.exp(x - maxx) / sumex
    return smx
```


[^1]: https://arxiv.org/pdf/2412.19437
[^2]: BF16 have 8 exponent bits, same as FP32. It's considered better than fp16 in model training. [Read More](https://stats.stackexchange.com/questions/637988/understanding-the-advantages-of-bf16-vs-fp16-in-mixed-precision-training).
[^3]: https://arxiv.org/pdf/1805.02867