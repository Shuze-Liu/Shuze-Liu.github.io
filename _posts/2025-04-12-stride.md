---
layout: post
# marp: true
title:  "torch internal: decoupling view from underlying storage"
categories: jekyll update
published: true
---


- what we see: matrix of dimension `(N, C, H, W)`
- in reality: 1D memory array w/ offset zero and stride `(C*H*W, H*W, W, 1)`
- benefits: allow zero-copy operations like transpose,broadcast, slice or flipping. 


---

## offset and stride

- offset: offset from first element of 1darray to first element of ndarray
- stride: (s1, s2, ...) where `sk = #step` on underlying 1d array, to get to next element along the dimension k

Mapping from ndimensional index to raw index is

$$
\text{raw idx}= \text{stride}^T\cdot \text{index} + \text{offset}
$$

Given `index :: array<3, int>`, `stride :: array<3, int>`, correspondence between ndarray and underlying array is
``` python
x[i, j, k] = underlying[i * stride[0] + j * stride[1] + k * stride[2] + offset]
```
---
## Stride of tensor
Stride of row-major tensor is suffix-production of shape,

$$
\text{stride}_i = \prod_{i+1}^N \text{shape}_i
$$

``` python
x = torch.ones(3, 4, 5, 6)
ndim = len(x.shape)
x_stride = [1] * ndim
for i in reversed(range(0, ndim-1)):
    x_stride[i] = x_stride[i+1] * x.shape[i+1]
print(x.shape, x_stride, x.stride())
# torch.Size([3, 4, 5, 6]) [120, 30, 6, 1] (120, 30, 6, 1)
```

---

## Example: zero-copy slicing
Zero copy can be achieved by manipulating offset and stride along, w/o touching underlying array
``` python
x = torch.arange(12).reshape(3, -1)
print_T(x, x[1:2, 1:4])
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])
#   shape torch.Size([3, 4]) offset 0 stride: (4, 1), data_ptr=123878912
# tensor([[5, 6, 7]])
#   shape torch.Size([1, 3]) offset 5 stride: (4, 1), data_ptr=123878912
```
---
## Example: zero-copy broadcast
Broadcasting along a dimension is merely visit the same element again and again when advancing in that dimension, by changing stride resective dimension to 0
``` python
t = torch.arange(24).reshape(2, 3, 4)
x = torch.arange(4)
# tensor([0, 1, 2, 3])
#   shape torch.Size([4]) offset 0 stride: (1,), data_ptr=123979392
# tensor([[[0, 1, 2, 3],
#          [0, 1, 2, 3],
#          [0, 1, 2, 3]],

#         [[0, 1, 2, 3],
#          [0, 1, 2, 3],
#          [0, 1, 2, 3]]])
#   shape torch.Size([2, 3, 4]) offset 0 stride: (0, 0, 1), data_ptr=123979392
```
---
## Example: zero-copy transposing (`permute(x, y, z)`)
Shuffle stride such that index of the same element move from
`idx = offset + stride.dot(index)` to
`idx = offset + stride.dot(index[x, y, z])`

Reference implementation: 
``` python
newshape = (torch.Tensor(tuple(x.shape)).int())[perm]
newstride = (torch.Tensor(x.stride()).int())[perm]
perm = x.as_strided(tuple(newshape), tuple(newstride), x.storage_offset())
```
---
## Example: flipping w/ negative stride[^1]

``` python
x = np.arange(12).reshape(3, 4)
print_T(x, np.flip(x, axis=[0]), np.flip(x, axis=[1]))
# [[ 0  1  2  3]
#  [ 4  5  6  7]
#  [ 8  9 10 11]]
#   shape (3, 4) offset 0 stride: [4, 1], dtype: int64
# [[ 8  9 10 11]
#  [ 4  5  6  7]
#  [ 0  1  2  3]]
#   shape (3, 4) offset 8 stride: [-4, 1], dtype: int64
# [[ 3  2  1  0]
#  [ 7  6  5  4]
#  [11 10  9  8]]
#   shape (3, 4) offset 3 stride: [4, -1], dtype: int64
```

---
## Appendix: printing stride and offset
``` python
def print_T(*xs):
    if not isinstance(xs, list) and not isinstance(xs, tuple):
        xs = [xs,]
    for x in xs:
        if isinstance(x, torch.Tensor):
            print(f'{x}\n  shape {x.shape} offset {x.storage_offset()} stride: {x.stride()}, data_ptr={x.storage().data_ptr()}')
        else: # numpy
            offset = (x.__array_interface__['data'][0] - x.base.__array_interface__['data'][0]) // x.itemsize if x.base is not None else 0
            print(f'{x}\n  shape {x.shape} offset {offset} stride: {(np.array(x.strides) // 8).tolist()}, dtype: {x.dtype}')
```





[^1]: numpy support negative stride while pytorch doesn't 