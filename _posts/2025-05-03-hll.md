---
layout: post
title:  "Set cardinality estimation in O(n) time and O(1) space"
categories: jekyll update
published: true
---
How to count distinct number of distinct elements from a stream? 
With the help of hash table, the problem can be solved in two linear passes:
- pass 1: inserting each element into hash map, which takes O(N) time
- pass 2: scanning through the map to find how many bin have been occupied, which is still a O(N) operation

O(N) time complexity sounds good enough, but the memory it required is also O(N), which is not ideal when there might be trillions of elements to count. 

Luckily, a O(N) time and O(1) space algorithm is possible if we give up some accuracy and estimate the size with probabilistic algorithm.
#### Flipping coin to counting
The story starts from flipping coins. Suppose n fair coins are repeatedly tossed until all tails appear. How many times does it take to stop on average? 

This can be modeled with [geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) of k

$$
P(X=k)=(1-p)^{k-1} * p
$$

Expected value of number of retries is $$E(X)=1/p$$.

The probability to get n tails when tossing n fair coins is `1/2^n`, so expected times of "re-toss" is `2^n`.

If the hash function is good enough, each 64 bit hash value it generates can be considered as tossing 64 coins. 
For a sequence of hashed values, the maximum number of tailing zeros can be seen as event of "k-tails", and hence we could estimate count of distinct elements with
`2^max tailing zero counts`
#### What if we got unlucky?
Since we are taking the maximum of tailing zeros, a single outlier can bring huge error to our estimation. So current algorithm is not robust enough to become useful. In probabilistic term, current estimator come with huge variance.  

When it comes to avoiding huge variance in estimation, a natural idea is repeat estimation multiple times and take an average of results. Since our estimation is deterministic, simple re-computing doesn't help, and re-hashing all sequence is costly. 

Inspired by hash bins, we can mimic the repetition by dividing elements into bins, make estimation of elements by bins and then take average of result as our final result. To mitigate effect of large positive outlier, harmonic mean is selected as average function. 

In math formula, the result is given by

$$
\hat{n}=\alpha_m \frac{m}{\frac{1}{m2^{K_1}}+\cdots+\frac{1}{m2^{K_m}}}
=\frac{\alpha_m m^2}{\sum_{i=1}^m2^{-K_i}}
$$

Where 
- $$m$$ is number of bin, 
- $$K_i$$ is the position of position of first '1' before tailing zero
- $$\alpha_m$$ is a bias-correcting constant[^1].

#### Result and Implementation
With 1e5 random samples, the average accuracy is `0.9939`, which is surprisingly accurate.
``` python
[100000/estimate_cardinality([random.random() for _ in range(100000)], 10) for _ in range(10)]
```
I borrow the code from blog[^2].

``` python
import mmh3
import random
def trailing_zeroes(num):
    if num == 0:
        return 32
    p = 0
    while (num >> p) & 1 == 0:
        p += 1
    return p

def estimate_cardinality(values, k):
    num_buckets = 2**k # use 2**k buckets
    max_zeroes = [0] * num_buckets # counting max tailing zero of bucket
    for value in values:
        h = mmh3.hash(str(value))
        bucket = h & (num_buckets - 1) 
        bucket_hash = h >> k
        max_zeroes[bucket] = max(max_zeroes[bucket], trailing_zeroes(bucket_hash) + 1)
    return 0.72134752 * (num_buckets ** 2) / sum([2 ** (-ki) for ki in max_zeroes])
```
Reference: 

[^1]: hyperloglog paper [link](https://oertl.github.io/hyperloglog-sketch-estimation-paper/paper/paper.pdf)

[^2]: blog [link](http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation)